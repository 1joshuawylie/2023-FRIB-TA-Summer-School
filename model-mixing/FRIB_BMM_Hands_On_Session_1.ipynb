{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399452fb",
   "metadata": {
    "id": "399452fb"
   },
   "source": [
    "# Taweret: Multivariate Model Mixing Tutorial\n",
    "\n",
    "## Author: Alexandra Semposki\n",
    "\n",
    "### Date: 26 June 2023\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the FRIB summer school hands-on session with our Bayesian Model Mixing (BMM) package, Taweret! Below we'll look at an interesting toy model that we can apply one of Taweret's many BMM methods to. We'll have short tasks for you along the way so that you can get accustomed to the package and to BMM generally.  \n",
    "\n",
    "---\n",
    "\n",
    "Further reading:\n",
    "\n",
    "*Bayesian data analysis and statistics*\n",
    "\n",
    "- A. Gelman, J. Carlin, H. Stern, D. Dunson, A. Vehtari, D. Rubin. \"Bayesian Data Analysis\", 1995. http://www.stat.columbia.edu/~gelman/book/.\n",
    "- J. S. Sivia, \"Data Analysis: A Bayesian Tutorial\", Oxford University Press, 2nd Ed., 2006. \n",
    "\n",
    "*Gaussian Processes*\n",
    "\n",
    "- J. A. Melendez, R. J. Furnstahl, D. R. Phillips, M. T. Pratola, and S. Wesolowski, Quantifying Correlated Truncation Errors in Effective Field Theory, Phys. Rev. C 100, 044001 (2019), arXiv:1904.10581 [nucl-th].\n",
    "- Carl Edward Rasmussen and Christopher K. I. Williams, \"Gaussian Processes for Machine Learning\", The MIT Press, 2006.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Below we define the models for our Bayesian Model Mixing exercise. The workflow for this hands-on tutorial is as follows:\n",
    "\n",
    "1). __Define__ the models;\n",
    "\n",
    "2). __Plot__ the models' means and standard deviations as uncertainty bands;\n",
    "\n",
    "3). Apply __BMM__ via the `Multivariate` method in Taweret;\n",
    "\n",
    "4). Plot the __posterior predictive distribution__ obtained from the model mixing;\n",
    "\n",
    "5). Examine the __weights__ of each individual model when compiled into the mixed model. \n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Our learning objectives include:\n",
    "- Understanding the toy model case used below;\n",
    "- Learning how to apply Multivariate Model Mixing using Taweret;\n",
    "- Developing a GP model and applying it to this toy model;\n",
    "- Interpreting the results of the final mixed model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb24ec",
   "metadata": {
    "id": "c8fb24ec"
   },
   "source": [
    "## Defining our models\n",
    "\n",
    "Now, let's start by looking at the models that we'll be using. We want to mix the expansions of the zero-dimensional $\\phi^4$-theory partition function, below:\n",
    "\n",
    "$$\n",
    " F(g) = \\int_{-\\infty}^{\\infty} dx~ e^{-\\frac{x^{2}}{2} - g^{2} x^{4}} = \\frac{e^{\\frac{1}{32 g^{2}}}}{2 \\sqrt{2}g} K_{\\frac{1}{4}}\\left(\\frac{1}{32 g^{2}} \\right).\n",
    "$$\n",
    "\n",
    "The two expansions are limits taken at $g = 0$ and $g = \\infty$:\n",
    "\n",
    "$$\n",
    "F_{s}^{N_s}(g) = \\sum_{k=0}^{N_{s}} s_{k} g^{k},\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "F_{l}^{N_{l}}(g) = \\frac{1}{\\sqrt{g}} \\sum_{k=0}^{N_{l}} l_{k} g^{-k},\n",
    "$$\n",
    "\n",
    "with coefficients given as:\n",
    "\n",
    "$$\n",
    "s_{2k} = \\frac{\\sqrt{2} \\Gamma{(2k + 1/2)}}{k!} (-4)^{k},~~~~~s_{2k + 1} = 0\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "l_{k} = \\frac{\\Gamma{\\left(\\frac{k}{2} + \\frac{1}{4}\\right)}}{2k!} \\left(-\\frac{1}{2}\\right)^{k}.\n",
    "$$\n",
    "\n",
    "Notice that $F_{s}^{N_{s}}(g)$ is an asymptotic series---the more terms we add to the series expansion, the more it diverges from the true solution. Hence, its radius of convergence is zero. However, $F_{l}^{N_{l}}(g)$ is a convergent series, hence its radius of convergence is $\\infty$, and we see in the plot that, with more orders added, we converge more and more with the true solution.\n",
    "\n",
    "__Checkpoint__: Click on the following link and scroll down on the README page to see an animated plot of different truncation orders!\n",
    "\n",
    "https://github.com/asemposki/SAMBA/tree/Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6e4af",
   "metadata": {
    "id": "b9b6e4af"
   },
   "source": [
    "We begin by importing all of the Python packages we will need in this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e423e2f",
   "metadata": {
    "id": "7e423e2f"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import statistics\n",
    "from cycler import cycler\n",
    "from scipy import stats, special, integrate\n",
    "\n",
    "# install a couple of needed packages for SAMBA\n",
    "!pip install emcee\n",
    "!pip install corner\n",
    "\n",
    "!pip install latex\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "%matplotlib inline\n",
    "\n",
    "#matplotlib settings for Latex plots\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': False,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb91adad",
   "metadata": {
    "id": "fb91adad"
   },
   "source": [
    "Now we pull in the Taweret package and the models it contains. Below we import from Taweret the __BaseModel__ and __BaseMixer__ classes, which we discussed during the first part of the session. They are the base classes on which Taweret operates, and which allow the user to define models and mixing strategies of their own!\n",
    "\n",
    "We are also importing from the `samba_models` file, which contains the toy model we're using. \n",
    "\n",
    "After we clone in Taweret, we'll need to clone in SAMBA for the (optional!) GP section of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KvqpqBP7xiN1",
   "metadata": {
    "id": "KvqpqBP7xiN1"
   },
   "outputs": [],
   "source": [
    "# Clone the Taweret Repo\n",
    "!git clone https://github.com/asemposki/Taweret.git \n",
    "!cd Taweret && git checkout develop\n",
    "\n",
    "sys.path.insert(0,'/content/Taweret')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VY-Epe2u0rCo",
   "metadata": {
    "id": "VY-Epe2u0rCo"
   },
   "outputs": [],
   "source": [
    "# Clone the SAMBA repo\n",
    "!git clone https://github.com/asemposki/SAMBA.git\n",
    "!cd SAMBA && git checkout Development\n",
    "\n",
    "sys.path.insert(0,'/content/SAMBA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247e08b",
   "metadata": {
    "id": "b247e08b"
   },
   "outputs": [],
   "source": [
    "sys.path\n",
    "\n",
    "from Taweret.Taweret.core.base_model import BaseModel\n",
    "from Taweret.Taweret.core.base_mixer import BaseMixer\n",
    "\n",
    "from Taweret.Taweret.models import samba_models  # problem right here\n",
    "from SAMBA.samba.gaussprocess import GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f19aea",
   "metadata": {
    "id": "57f19aea"
   },
   "source": [
    "## Plotting our models\n",
    "\n",
    "Now we plot the functions we want to mix to see what they look like. We need to first pick a value for $N_{s}$ and $N_{l}$. Let's pick $N_{s}$ = 3 and $N_{l}$ = 3, which means we truncate our series expansions after the third term in each. Our uncertainties are then $\\mathcal{O}(g^{4})$.\n",
    "\n",
    "We must also define the range of our input parameter (coupling constant), $g$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b60813",
   "metadata": {
    "id": "28b60813"
   },
   "outputs": [],
   "source": [
    "#define g and orders of the series expansions\n",
    "g = np.linspace(1e-6,1.0,100)\n",
    "N_s = 3\n",
    "N_l = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e9828",
   "metadata": {
    "id": "038e9828"
   },
   "source": [
    "To plot our models and their uncertainties, we look at what functions are available to us in the `samba_models` file. __Uncomment the cell below and run it. What functions are contained in the file?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a31d15",
   "metadata": {
    "id": "97a31d15",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(samba_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OfdyfO2V9LyJ",
   "metadata": {
    "id": "OfdyfO2V9LyJ"
   },
   "source": [
    "We will first make a `dict` of models and then plot them, calling the classes listed in the `help` information above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f9d6ca",
   "metadata": {
    "id": "82f9d6ca"
   },
   "outputs": [],
   "source": [
    "# call the 'loworder' class to generate the small-g expansion model given N_s\n",
    "model_1 = samba_models.loworder(N_s, error_model='informative')\n",
    "\n",
    "# call the 'highorder' class to generate the large-g expansion model given N_l\n",
    "model_2 = samba_models.highorder(N_l, error_model='informative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a371c",
   "metadata": {
    "id": "393a371c"
   },
   "outputs": [],
   "source": [
    "# define the dictionary of models\n",
    "my_models = {\n",
    "    \"1\" : model_1,\n",
    "    \"2\" : model_2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb22937",
   "metadata": {
    "id": "eeb22937"
   },
   "source": [
    "This is great, but how do we *plot* the models using the linspace in $g$ that we specified earlier? We use the function `evaluate`, located in each of the models' classes above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47a2f3",
   "metadata": {
    "id": "5c47a2f3"
   },
   "outputs": [],
   "source": [
    "#help(samba_models.loworder.evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b1f164",
   "metadata": {
    "id": "d9b1f164"
   },
   "source": [
    "__Checkpoint__: Uncomment the cell above and run it. What two outputs does the `evaluate` function return?\n",
    "\n",
    "Let's evaluate both of our models from the `dict` above, and plot them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d754bc0",
   "metadata": {
    "id": "7d754bc0"
   },
   "outputs": [],
   "source": [
    "# evaluate the two models\n",
    "model_results = []\n",
    "\n",
    "for i in my_models.keys():\n",
    "    model_results.append(my_models[i].evaluate(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207cdae",
   "metadata": {
    "id": "f207cdae"
   },
   "outputs": [],
   "source": [
    "# plot the models\n",
    "fig = plt.figure(figsize=(8,6), dpi=150)\n",
    "ax = plt.axes()\n",
    "ax.set_xlim(0.0,1.0)\n",
    "ax.set_ylim(1.0,3.0)\n",
    "ax.text(0.9,0.9, r'$F(g)$: Models', ha='right', va='top',\n",
    "        transform=ax.transAxes, fontsize=16)\n",
    "ax.set_xlabel(r'$g$')\n",
    "ax.set_ylabel(r'$F(g)$')\n",
    "\n",
    "# color scheme\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "# labels\n",
    "labels = [r'$N_s = {}$'.format(N_s), r'$N_l = {}$'.format(N_l), r'Added model']\n",
    "unc_labels = [r'68% CI', r'68% CI', r'68% CI']\n",
    "\n",
    "# lines\n",
    "lines = ['dashed', 'dotted']\n",
    "\n",
    "# plot the true model\n",
    "ax.plot(g, samba_models.true_model().evaluate(g)[0].flatten(), color='k', label='True')\n",
    "\n",
    "# model means\n",
    "for i in range(len(model_results)):\n",
    "      ax.plot(g, model_results[i][0].flatten(), color=colors[i], linestyle=lines[0], label=labels[i])\n",
    "        \n",
    "# model uncertainties\n",
    "for i in range(len(model_results)):\n",
    "    ax.plot(g, model_results[i][0].flatten() - model_results[i][1].flatten(), color=colors[i], linestyle=lines[1], label=unc_labels[i])\n",
    "    ax.plot(g, model_results[i][0].flatten() + model_results[i][1].flatten(), color=colors[i], linestyle=lines[1])\n",
    "    \n",
    "# legend\n",
    "ax.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wZj8GuPTB5HD",
   "metadata": {
    "id": "wZj8GuPTB5HD"
   },
   "source": [
    "__Checkpoint__: What do the dotted curves above represent? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d9c374",
   "metadata": {
    "id": "70d9c374"
   },
   "source": [
    "Now we need a way to estimate the truncation error from the excluded terms in the series expansions. \n",
    "\n",
    "We can consider that we may have some intuition, based on the coefficients we do know, for those at higher orders that we do not possess. As we know that the small-$g$ expansion above (in red) is an asymptotic series, and that the large-$g$ expansion (in blue) is a converging Taylor series, we could form coefficients that follow the series' expansion parameters like so:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma_{N_s}(g)=\\left\\{ \\begin{array}{lc}\n",
    "\t\\Gamma(N_s/2+1) (4g)^{N_s + 2} \\bar{c}, & \\mbox{if $N_s$ is even;}\\\\\n",
    "    \\Gamma(N_s/2+1/2) (4g)^{N_s+1} \\bar{c}, & \\mbox{if $N_s$ is odd,}\n",
    "    \\end{array} \\right. \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "for the small-$g$ expansion, and \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma_{N_l}(g)=\\left(\\frac{1}{4g}\\right)^{N_l + 3/2} \\frac{1}{\\Gamma(N_l/2+3/2)} \\bar{d},\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "for the large-$g$ expansion. We could also assume that we know very little, and come up with a model that is even less informative than this one. The informative example above, however, is the one that we have set as a default in the models used in Taweret (this of course can be changed if you wish!).\n",
    "\n",
    "For all of the nitty gritty details, see Appendix B of [our paper](https://arxiv.org/abs/2206.04116)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643d408",
   "metadata": {
    "id": "c643d408"
   },
   "source": [
    "## Part I: Bivariate Mixing\n",
    "\n",
    "To mix our models, we form Gaussian distributions first from $F_{s}(g)$ and $F_{l}(g)$:\n",
    "\n",
    "$$\n",
    "\\mathcal{M}_{s} = \\mathcal{N}(F_{s}(g), \\sigma^{2}_{s}(g)), \\quad\n",
    "\\mathcal{M}_{l} = \\mathcal{N}(F_{l}(g), \\sigma^{2}_{l}(g)).\n",
    "$$\n",
    "\n",
    "From here, we can actually simply mix these two models together! Using Taweret, we can call the model mixing scheme that will mix the two models using the bivariate model mixing method, where we combine the two models' means and variances like so:\n",
    "\n",
    "$$\n",
    "f^{\\dagger} \\sim \\bigl(Z_P^{-1}\\sum_k \\frac{1}{v_k}f_k, Z_P^{-1}\\bigr),\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "Z_P \\equiv \\sum_{k=1}^{K}\\frac{1}{v_k}.\n",
    "$$\n",
    "\n",
    "$Z_P$ is the precision, or the inverse of the variance, of the $K$ models, $v_{k}$ the individual variances of each model (which we previously denoted the theory error), and $f^{\\dagger}$ the mixed model.\n",
    "\n",
    "To mix these models together, we need to instantiate the `Multivariate` class in Taweret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260f65d",
   "metadata": {
    "id": "5260f65d"
   },
   "outputs": [],
   "source": [
    "from Taweret.mix.gaussian import Multivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HYhEKdgZ8B2e",
   "metadata": {
    "id": "HYhEKdgZ8B2e"
   },
   "source": [
    "__Checkpoint__: Look over the `Multivariate` class using the commented help cell below. What functions does it contain? What do they do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9753f",
   "metadata": {
    "id": "dfa9753f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(Multivariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a3ca1",
   "metadata": {
    "id": "f19a3ca1"
   },
   "outputs": [],
   "source": [
    "# call Multivariate\n",
    "biv_model = Multivariate(g, my_models, n_models=len(my_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081dca26",
   "metadata": {
    "id": "081dca26"
   },
   "source": [
    "Now we simply use the `predict` function in the `Multivariate` class to produce the mixed model with its mean and uncertainties. We can plot this result below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s-8NvKclFE_X",
   "metadata": {
    "id": "s-8NvKclFE_X"
   },
   "source": [
    "__Checkpoint__: Uncomment the cell below and run it. What inputs and outputs does `Multivariate.predict` have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sv0-bnGE8StT",
   "metadata": {
    "id": "sv0-bnGE8StT"
   },
   "outputs": [],
   "source": [
    "#help(Multivariate.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d07d2",
   "metadata": {
    "id": "ea9d07d2"
   },
   "outputs": [],
   "source": [
    "# call predict\n",
    "_, biv_mean, biv_intervals, biv_std_dev = biv_model.predict(ci=68)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqLx_k968cnT",
   "metadata": {
    "id": "aqLx_k968cnT"
   },
   "source": [
    "Now we plot the mixed model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaaec35",
   "metadata": {
    "id": "3eaaec35"
   },
   "outputs": [],
   "source": [
    "# plot the mixed model\n",
    "fig = plt.figure(figsize=(8,6), dpi=150)\n",
    "ax = plt.axes()\n",
    "ax.set_xlim(0.0,1.0)\n",
    "ax.set_ylim(1.0,3.0)\n",
    "ax.text(0.9,0.9, r'$F(g)$: PPD, Bivariate MM', ha='right', va='top',\n",
    "        transform=ax.transAxes, fontsize=16)\n",
    "ax.set_xlabel(r'$g$')\n",
    "ax.set_ylabel(r'$F(g)$')\n",
    "ax.plot(g, samba_models.true_model().evaluate(g)[0].flatten(), color='k', label='True')\n",
    "\n",
    "# model means\n",
    "for i in range(len(model_results)):\n",
    "      ax.plot(g, model_results[i][0].flatten(), color=colors[i], linestyle=lines[0], label=labels[i])\n",
    "\n",
    "# PPD mean\n",
    "ax.plot(g, biv_mean, color=colors[2], label='PPD')\n",
    "        \n",
    "# model uncertainties\n",
    "for i in range(len(model_results)):\n",
    "    ax.plot(g, model_results[i][0].flatten() - model_results[i][1].flatten(), color=colors[i], linestyle=lines[1])\n",
    "    ax.plot(g, model_results[i][0].flatten() + model_results[i][1].flatten(), color=colors[i], linestyle=lines[1])\n",
    "    \n",
    "# PPD band\n",
    "ax.fill_between(g, biv_mean-biv_std_dev, biv_mean+biv_std_dev,\n",
    "                        zorder=i-5, facecolor=colors[2], edgecolor=colors[2], alpha=0.2)\n",
    "\n",
    "# legend\n",
    "ax.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f4ee44",
   "metadata": {
    "id": "92f4ee44"
   },
   "source": [
    "And, as simple as that, there is the mixed model! The green curve is the PPD from the $f^{\\dagger}$ equation previously discussed. \n",
    "\n",
    "__Question__: Discuss with your neighbour whether you think that the uncertainty band in the ''gap'' of the mixed model plot is too large, too small, or reasonable. Speculate on how, in a real-world scenario where the true model was *not* a known quantity, you might know this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f800c",
   "metadata": {
    "id": "2f4f800c"
   },
   "source": [
    "### Weights: bivariate MM\n",
    "\n",
    "Now we'd like to look at what the location-dependent weights show us. What are the __weights__ of the models? These are simply the inverse of their variances (the precision, to be precise). These weights show which model dominates in which region of the input space, and they do so point-by-point, having been calculated at each chosen point in the input space. They are plotted below for the two models above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U9SngoVHMMul",
   "metadata": {
    "id": "U9SngoVHMMul"
   },
   "source": [
    "__Exercise__: Fill in the code to evaluate the weights by calling the necessary function from the `Multivariate` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4662781",
   "metadata": {
    "id": "b4662781"
   },
   "outputs": [],
   "source": [
    "# call the evaluate_weights function\n",
    "biv_weights = ### YOUR CODE HERE ###\n",
    "\n",
    "labels_weights = [r'$1/\\sigma_{s}$', r'$1/\\sigma_{l}$', r'$1/\\sigma_{\\textrm{added_model}}$']\n",
    "\n",
    "# plot the weights\n",
    "fig = plt.figure(figsize=(8,6), dpi=150)\n",
    "ax = plt.axes()\n",
    "ax.set_xlim(0.0,1.0)\n",
    "ax.set_ylim(0.0,1.0)\n",
    "ax.text(0.9,0.9, 'Weights: Bivariate MM', ha='right', va='top',\n",
    "        transform=ax.transAxes, fontsize=16)\n",
    "ax.set_xlabel(r'$g$')\n",
    "ax.set_ylabel('Weights')\n",
    "\n",
    "# model weights\n",
    "for i in range(len(biv_weights)):\n",
    "    ax.plot(g, biv_weights[i], color=colors[i], label=labels_weights[i])\n",
    "\n",
    "# legend\n",
    "ax.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d50aa0",
   "metadata": {
    "id": "e3d50aa0"
   },
   "source": [
    "__Questions__: What do the weights above tell you? Do these results make sense, based what you know about which model is dominant in each region?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0dd34",
   "metadata": {
    "id": "77f0dd34"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a45b2a1",
   "metadata": {
    "id": "0a45b2a1"
   },
   "source": [
    "## Challenge questions for Part I\n",
    "\n",
    "As previously mentioned, the mixed model result for the two toy models above could use some thought, and perhaps, some improvement.\n",
    "\n",
    "__Questions for you to think about and explore__:\n",
    "\n",
    "1). What do you think would happen if you instead used a different truncation order for $N_{l}$? Try using $N_{l} = 6$ and see what happens. How does the mean of the mixed model change? How do the weights change for this new order?\n",
    "\n",
    "2). __Challenge exercise__: Try adding another model to the mix in the `my_models` dictionary. You can add either a small-$g$ (`loworder`) or large-$g$ (`highorder`) model. (Taweret is currently capable of handling $N$ models when using this mixing technique!) What happens to the mixed model mean, uncertainty, and the weights of each model when you do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hnViUE4qeCQb",
   "metadata": {
    "id": "hnViUE4qeCQb"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc86bfa",
   "metadata": {
    "id": "dcc86bfa"
   },
   "source": [
    "## Part II: Multivariate Model Mixing: Adding a GP\n",
    "\n",
    "We'd like to improve our results above, so we will employ a __Gaussian Process__. Gaussian processes are non-parametric emulators that can be used for interpolation and regression. They possess tunable hyperparameters that we can calibrate to improve the GP fit and predict capabilities. \n",
    "\n",
    "Much like a neural network (and in fact it is the limit of one) a GP requires training data to calibrate it, and testing data to predict results at new data points. Their form consists of a mean function, $m(x)$, and a covariance function, $\\kappa(x,x')$. Usually the mean function is taken to be zero, though that can be altered if needed. The covariance function is usually a function of the Euclidean distance between the points $x$ and $x'$, which we term __stationary kernels__ of our GP. (As you can probably imagine, a covariance function that does *not* depend on |$x-x'$| but instead on $x$ is a __non-stationary kernel__, but we don't need to worry about these *here*.) From the name, the covariance function is the crucial component of the GP that captures correlations in the data, and must be positive semi-definite. We can mathematically write a GP down as\n",
    "\n",
    "$$\n",
    "\\textbf{f}|\\textbf{x} = \\mathcal{N}(\\textbf{m}, \\textit{K}),\n",
    "$$\n",
    "where $\\textbf{m} = m(\\textbf{x})$ and $\\textit{K} = \\kappa(\\textbf{x}, \\textbf{x}')$, and $\\textbf{f}$ is a training set of data corresponding to training points $\\textbf{x}$.\n",
    "\n",
    "Here, we employ the well-known Python package `scikit-learn`, which allows the user the pick from multiple forms of the covariance function to set the __prior__ of the GP. Let's look at some of the options there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb61d45f",
   "metadata": {
    "id": "eb61d45f"
   },
   "outputs": [],
   "source": [
    "# import scikit learn\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process import kernels\n",
    "\n",
    "# show the options for kernels by uncommenting and running the line below\n",
    "#help(kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170af5c9",
   "metadata": {
    "id": "170af5c9"
   },
   "source": [
    "Looking above through the `help` information, we can spot the types of stationary kernels scikit-learn can call and implement: \n",
    "- ConstantKernel\n",
    "- RBF\n",
    "- ExpSineSquared\n",
    "- RationalQuadratic\n",
    "- Matern\n",
    "- WhiteKernel\n",
    "\n",
    "Let's pull some samples from one of them to see what the form of the kernel looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9792845c",
   "metadata": {
    "id": "9792845c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set up the RBF kernel and GP object\n",
    "kern = kernels.RBF(length_scale=0.4)\n",
    "gpr = GaussianProcessRegressor(kern)\n",
    "\n",
    "x = np.linspace(0.0,1.0,500)\n",
    "X = x[:,None]\n",
    "\n",
    "# sample from the prior (kernel)\n",
    "n_samples =   ### YOUR CODE HERE ### \n",
    "samples = gpr.sample_y(X, n_samples=n_samples)\n",
    "\n",
    "# plot these samples\n",
    "fig = plt.figure(figsize=(8,6), dpi=150)\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Samples')\n",
    "ax.set_xlim(0.0,1.0)\n",
    "ax.set_ylim(-3.0,3.0)\n",
    "ax.set_title('Sampled curves: RBF Kernel')\n",
    "    \n",
    "# plot the mean and variance levels\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axhline(y=2, color='grey')\n",
    "ax.axhline(y=-2, color='grey')\n",
    "\n",
    "# plot the samples\n",
    "for i in range(n_samples):\n",
    "    ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a273db",
   "metadata": {
    "id": "88a273db"
   },
   "source": [
    "The grey lines correspond to the 2$\\sigma$ variance level, and the mean function is at y = 0 (black line). From this sampling, we can see that the curves from the RBF kernel are very smooth---*too* smooth, in fact, for some applications, as we'll see later on.\n",
    "\n",
    "__Exercise__: Try sampling curves from the Matern 3/2 kernel instead. What do the curves look like? How are they different from those sampled from the RBF kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6czKiqwN08B",
   "metadata": {
    "id": "c6czKiqwN08B"
   },
   "source": [
    "Now, let's try to use one of these vanilla GPs to interpolate between the two models in the toy case, and see how our results change. Why would we want to do this? Because GPs can be used to build in important physics and constraints on the system, which would be very useful in a real-world application---*especially* in regions where we may not have much data.\n",
    "\n",
    "How do we include the GP as another model in the mixing strategy? We simply use the formula defined at the top of this notebook, which we copy here again:\n",
    "\n",
    "$$\n",
    "f^{\\dagger} \\sim \\bigl(Z_P^{-1}\\sum_k \\frac{1}{v_k}f_k, Z_P^{-1}\\bigr),\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "Z_P \\equiv \\sum_{k=1}^{K}\\frac{1}{v_k}.\n",
    "$$\n",
    "\n",
    "$Z_P$ is the precision, or the inverse of the variance, of the $K$ models, $v_{k}$ the individual variances of each model (which we previously denoted the theory error), and $f^{\\dagger}$ the mixed model. For our specific case, $K=3$ now, since we're including the GP as a third model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086e439",
   "metadata": {
    "id": "0086e439"
   },
   "source": [
    "### Writing a model class for the GP\n",
    "\n",
    "Now it's your turn to shine! In this section, you'll see a lot of empty code blocks---__you__ get to fill those in and develop the model wrapper class for the GP model. \n",
    "\n",
    "To use a GP as a third model, we need to instantiate a model class that includes the GP and conforms to the necessary `BaseModel` requirements. You'll start by filling in the `__init__` function to call the GP class developed in the package `SAMBA` which has been loaded into this notebook. The GP we have will be instantiated in the `__init__` function, and the `evaluate` function will train, test, and output results from the GP in the form of the model mean and variance.  \n",
    "\n",
    "*Hint for building the `evaluate` function: Ignore the static methods at the bottom of the help information and focus on the class functions at the top. Which of these are necessary to implement a GP given training and testing data?*\n",
    "\n",
    "__Bonus information on GPs__:\n",
    "In `scikit-learn`, GPs are instantiated via the `GaussianProcessRegressor` class. It takes the arguments `kernel`, which we already know, `n_restarts_optimizer`, which indicates how many different starting points to use when optimizing the hyperparameters, and `normalize_y`, a Boolean variable that determines whether $m(x)$ (the mean function) is zero or the mean of the data set we use. All of this is wrapped below in the GPModel class. If you would like to play with these parameters, or just look at how we formulate this GP code in more detail, check out the SAMBA package we're wrapping below [here](https://github.com/asemposki/SAMBA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0Hv6fQtcXCyU",
   "metadata": {
    "id": "0Hv6fQtcXCyU"
   },
   "outputs": [],
   "source": [
    "# uncomment and run this cell to see the SAMBA GP options\n",
    "#help(GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c371cca3",
   "metadata": {
    "id": "c371cca3"
   },
   "outputs": [],
   "source": [
    "# begin the model class with an __init__ function that takes\n",
    "# the kernel and the input space range (see BaseModel for functions needed)\n",
    "class GPModel: \n",
    "    \n",
    "    # init function sets up the GP\n",
    "    # nu = value for Matern kernel (root of Bessel function)\n",
    "    def __init__(self, g, N_s, N_l, kernel='RBF', nu=0, ci=68, \\\n",
    "                 error_model='informative'):\n",
    "        \n",
    "        self.g = g    \n",
    "        self.N_s = N_s\n",
    "        self.N_l = N_l\n",
    "        \n",
    "        # set up the GP class object with our model orders and chosen kernel\n",
    "        self.gpmodel = ### YOUR CODE HERE ###\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # evaluate function for fitting and predicting\n",
    "    # call GP functions to train and test your GP and output results\n",
    "    def evaluate(self, g):\n",
    "    \n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        return mean, std_dev\n",
    "    \n",
    "    # to comply with BaseModel, write the log_likelihood function\n",
    "    def log_likelihood_elementwise(self):\n",
    "        return None\n",
    "    \n",
    "    # to comply with BaseModel, write the set_prior function\n",
    "    def set_prior(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e9c6d",
   "metadata": {
    "id": "ae0e9c6d"
   },
   "source": [
    "Because the toy models we are using vary greatly in the speed with which they approach $\\pm \\infty$ depending on the truncation orders chosen, we must use a detailed training set generator function that determines where to place the points for the GP to train on without straying too far into the region where we do not know much about the function (the ''gap'' in the middle of the plot from previously). Hence, we're wrapping our code from [our paper](https://arxiv.org/abs/2206.04116) to do this quickly here.\n",
    "\n",
    "Now we can call the above class and see what happens. Once we do this, we can store the mean and standard deviation from this model in the models `dict` and use Taweret to mix all three of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c662d6",
   "metadata": {
    "id": "92c662d6"
   },
   "outputs": [],
   "source": [
    "# call the GP model and set the object\n",
    "gp_model = GPModel(g, N_s, N_l, kernel='Matern', nu=1.5, ci=68)\n",
    "\n",
    "# redefine the dict of models to include the GP model\n",
    "my_models = ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd9e98d",
   "metadata": {
    "id": "2dd9e98d"
   },
   "source": [
    "Finally, we're ready to mix. Use the Taweret `Multivariate` class and its functions to mix the three models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d5d43",
   "metadata": {
    "id": "fe7d5d43",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# instantiate the Multivariate class\n",
    "mult_mixed = ### YOUR CODE HERE ###\n",
    "\n",
    "# obtain the posterior predictive distribution for the mixed model\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae32a57",
   "metadata": {
    "id": "4ae32a57"
   },
   "source": [
    "If you've written your `GPModel` class correctly, you should see plots of the training and validation sets when you run the mixed model prediction above, along with the values of the optimized kernel hyperparameters! \n",
    "\n",
    "__Note__: The above validation data does NOT have the mixed model included, this just shows the three models separately. We can plot the mixed PPD (posterior predictive distribution) and its credible intervals below. \n",
    "\n",
    "__Question__: Turn to your neighbour and tell them whether you think the kernel hyperparameters you obtained look reasonable or not given the toy models you mixed. *If you don't know what the kernel hyperparameters mean, ask!*\n",
    "\n",
    "Now we plot the mixed model and uncertainty band just as before. __Fill in the code to plot them below.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eefb90",
   "metadata": {
    "id": "b0eefb90"
   },
   "outputs": [],
   "source": [
    "# plot the mixed model including the GP\n",
    "fig = plt.figure(figsize=(8,6), dpi=150)\n",
    "ax = plt.axes()\n",
    "ax.set_xlim(0.0,1.0)\n",
    "ax.set_ylim(1.0,3.0)\n",
    "ax.text(0.9,0.9, r'$F(g)$: PPD, Multivariate MM', ha='right', va='top',\n",
    "        transform=ax.transAxes, fontsize=16)\n",
    "ax.set_xlabel(r'$g$')\n",
    "ax.set_ylabel(r'$F(g)$')\n",
    "ax.plot(g, samba_models.true_model().evaluate(g)[0].flatten(), color='k', label='True')\n",
    "\n",
    "# model means\n",
    "for i in range(len(model_results)):\n",
    "      ax.plot(g, model_results[i][0].flatten(), color=colors[i], linestyle=lines[0], label=labels[i])\n",
    "\n",
    "# PPD mean\n",
    "### YOUR CODE HERE ###\n",
    "        \n",
    "# model uncertainties\n",
    "for i in range(len(model_results)):\n",
    "    ax.plot(g, model_results[i][0].flatten() - model_results[i][1].flatten(), color=colors[i], linestyle=lines[1])\n",
    "    ax.plot(g, model_results[i][0].flatten() + model_results[i][1].flatten(), color=colors[i], linestyle=lines[1])\n",
    "    \n",
    "# PPD band (hint: use fill_between for a cleaner plot)\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# legend\n",
    "ax.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba9e97",
   "metadata": {
    "id": "75ba9e97"
   },
   "source": [
    "This is excellent! Now we have results for the PPD with a GP as an interpolant model between the two original toy models. We can check out the weights from these models to see what they look like. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7064a3",
   "metadata": {
    "id": "6a7064a3"
   },
   "source": [
    "### Weights: including a GP\n",
    "\n",
    "We use the same procedure as before and call the `evaluate_weights` function from the `Multivariate` Taweret class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ad585",
   "metadata": {
    "id": "368ad585",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# call the evaluate_weights function\n",
    "weights = ### YOUR CODE HERE ### \n",
    "\n",
    "labels_weights_mult = [r'$1/\\sigma_{s}$', r'$1/\\sigma_{l}$', r'$1/\\sigma_{GP}$']\n",
    "\n",
    "# plot the weights\n",
    "fig = plt.figure(figsize=(8,6), dpi=150)\n",
    "ax = plt.axes()\n",
    "ax.set_xlim(0.0,1.0)\n",
    "ax.set_ylim(0.0,1.0)\n",
    "ax.text(0.9,0.9, 'Weights: Multivariate MM', ha='right', va='top',\n",
    "        transform=ax.transAxes, fontsize=16)\n",
    "ax.set_xlabel(r'$g$')\n",
    "ax.set_ylabel('Weights')\n",
    "\n",
    "# model weights\n",
    "for i in range(len(weights)):\n",
    "    ax.plot(g, weights[i], color=colors[i], label=labels_weights_mult[i])\n",
    "\n",
    "# legend\n",
    "ax.legend(loc='center right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccae8ec",
   "metadata": {
    "id": "5ccae8ec"
   },
   "source": [
    "__Question__: Why do you think there are these giant bumps in the weights when looking at toy model orders of $N_{s} = N_{l} = 3$? Why are the curves not smooth in this regard? *Hint*: we have a pretty dense linspace here that we're using for the validation data and the training data. Following this, why is the large-$g$ model not taking over completely at $g = 1$? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a02030",
   "metadata": {
    "id": "34a02030"
   },
   "source": [
    "---\n",
    "\n",
    "## Part II: Concluding remarks and questions\n",
    "\n",
    "Now it's time for us to wrap this tutorial up! \n",
    "\n",
    "__Some final questions for you to answer:__\n",
    "\n",
    "1). When using $N_{s} = 3$ and $N_{l} = 3$, you can see that the curves dip down towards $-\\infty$. Why does this __not__ affect the result from Taweret? In other words, why does the PPD curve not dip with the models as it did in the previous example (bivariate mixing without a GP)?\n",
    "\n",
    "2). How does the result change if I use a different kernel than the Matern kernel, say the RBF one we explored earlier? Change the kernel to 'RBF' and see what happens!\n",
    "\n",
    "3). How does the result look if you change the truncation order of one or both of the models? For example, you can change $N_{s}$ to 5, and $N_{l}$ to 7, or any other desired combination. How do the weights change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ab0ba",
   "metadata": {
    "id": "b68ab0ba"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41887e4",
   "metadata": {
    "id": "d41887e4"
   },
   "source": [
    "Jupyter notebook written by: Alexandra Semposki\n",
    "\n",
    "SAMBA package written by: Alexandra Semposki\n",
    "\n",
    "Taweret package written by: Dan Liyanage, Alexandra Semposki, John Yannotty, and Kevin Ingles"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1KFTdPqY3w3fqTgAOSSXlUAgV4TTRUBWZ",
     "timestamp": 1686071548599
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
