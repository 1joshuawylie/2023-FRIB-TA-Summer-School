{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgNS_1qv-qg1"
   },
   "source": [
    "# Challenge Project #2: Nuclear Masses and BART\n",
    "\n",
    "### Author(s): John Yannotty, Alexandra Semposki, Rahul Jain\n",
    "\n",
    "### 26 June 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIVmbkhG-qhD"
   },
   "source": [
    "Welcome to one of the two challenge problems of the BMM session! You've made it to undiscovered country!\n",
    "\n",
    "Now that you've gained a good level of understanding from the first hands-on notebook, we'll expand to look at the application of Bayesian Model Mixing (BMM) to nuclear masses. Let's first do an import of the masses data and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1392,
     "status": "ok",
     "timestamp": 1687641397733,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "16RVUEPZ-qhF"
   },
   "outputs": [],
   "source": [
    "# required imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.stats import norm\n",
    "from scipy.special import gamma\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1687641399114,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "n7irBFk2-qhK"
   },
   "outputs": [],
   "source": [
    "# define plot function for nuclear masses\n",
    "def plot_nuclear(df,col):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    fig.set_dpi(150)\n",
    "    s = ax.scatter(df['N'], df['Z'], c=df[col],marker='s',label=col+('  (B.E. in MeV)'))\n",
    "    ax.set_xlabel('N (neutrons)')\n",
    "    ax.set_ylabel('Z (protons)')\n",
    "    ax.legend()\n",
    "    fig.colorbar(s, ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpyQQkLs-qhM"
   },
   "source": [
    "## Exploring nuclear mass data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1687641400799,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "54lD-rox-qhN"
   },
   "outputs": [],
   "source": [
    "# import nuclear masses from csv file\n",
    "url= 'https://raw.githubusercontent.com/ascsn/2023-FRIB-TA-Summer-School/main/model-mixing/BMM_Masses.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1687641402754,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "NPQDU6ocQhVR",
    "outputId": "9a3cc76e-dde9-4f43-b811-65be6717962a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(url)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVB0qVrb-qhP"
   },
   "source": [
    "OK, great! So how do we interpret the data we just imported?\n",
    "\n",
    "Check out the column headers; the __Z__ and __N__ columns indicate the number of protons and neutrons, respectively, in a given nucleus. The four other categories are averages over sets of models that can be described by the types of methods they use---for example, __Skyrme__ uses the Skyrme energy density functional.\n",
    "\n",
    "If you're very curious about the forms of these models individually, check out the reference list below.\n",
    "\n",
    "- Skyrme: Density Functional Theory models based on Skyrme Energy Density Functional\n",
    "    - SkM* - https://www.sciencedirect.com/science/article/pii/0375947482904031?via%3Dihub\n",
    "    - SkP - https://www.sciencedirect.com/science/article/pii/0375947484904330?via%3Dihub\n",
    "    - SLy4 - https://iopscience.iop.org/article/10.1088/0031-8949/1995/T56/034\n",
    "    - SV-min - https://journals.aps.org/prc/abstract/10.1103/PhysRevC.79.034310\n",
    "\n",
    "\n",
    "- UNEDF: Density Functional Theory models also based on Skyrme Energy Density Functional\n",
    "    - UNEDF0 - https://journals.aps.org/prc/abstract/10.1103/PhysRevC.82.024313\n",
    "    - UNEDF1 - https://journals.aps.org/prc/abstract/10.1103/PhysRevC.85.024304\n",
    "    - UNEDF2 - https://journals.aps.org/prc/abstract/10.1103/PhysRevC.89.054314\n",
    "\n",
    "\n",
    "- Gogny: Density Functional Theory models based on Gogny Energy Density Functional\n",
    "    - BCPM - https://journals.aps.org/prc/abstract/10.1103/PhysRevC.87.064305\n",
    "    - D1M - https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.102.242501\n",
    "\n",
    "\n",
    "- Astro: More phenomenological mass models commonly used in nuclear astrophysics\n",
    "    - FRDM2012 - https://www.sciencedirect.com/science/article/pii/S0092640X1600005X?via%3Dihub\n",
    "    - HFB24 - https://journals.aps.org/prc/abstract/10.1103/PhysRevC.88.024308\n",
    "    \n",
    "    \n",
    "Now we will plot one of the models over the nuclear chart, along with its uncertainties. These were assigned by averaging the individual model uncertainties within each of the groups of models above. The variable we're plotting in is the __binding energy__ in MeV (abbreviated B.E. in the plot legend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664
    },
    "executionInfo": {
     "elapsed": 1074,
     "status": "ok",
     "timestamp": 1687641407914,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "wChcu2Na-qhR",
    "outputId": "6ce43dcf-4f0f-489c-da76-f4f476f1d659"
   },
   "outputs": [],
   "source": [
    "plot_nuclear(df,'Skyrme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664
    },
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1687641411679,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "wKJqvNPZ-qhR",
    "outputId": "157332dd-8a1b-447a-ef81-f89ebbe71029"
   },
   "outputs": [],
   "source": [
    "plot_nuclear(df,'Skyrme_sd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0dljMtN-qhS"
   },
   "source": [
    "__Question__: You can see that the binding energy uncertainties above are very small for the line passing diagonally through the center of the nuclear chart band. Can you guess why that might be happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "524bMbg5-qhS"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-78Nvo1FDwkp"
   },
   "source": [
    "# Challenge Project 2: **BART-BMM: Mixing with 2-Dimensional Input Spaces**\n",
    "## **Introduction**\n",
    "\n",
    "[All BART descriptions and code below credited to John Yannotty]\n",
    "\n",
    "This part of the challenge uses Bayesian Additive Regression Trees, which are able to handle multidimensional inputs. This BMM approach leverages observational data and the physics-based predictions from each model to obtain better global prediction and interpretation of the physical system. In our case, this means that we'll be able to mix our four nuclear mass models over the entire nuclear chart in both $N$ and $Z$ instead of just looking at a slice of one or the other.\n",
    "$\\newcommand{\\xvec}{\\boldsymbol x}$ $\\newcommand{\\wvec}{\\boldsymbol w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILrhfRkyAHHv"
   },
   "source": [
    "## **The BART-BMM Model**\n",
    "\n",
    "Consider $K$ physics models, each intended to explain the underlying physical system $f_\\dagger(\\xvec)$ across different subregions of the domain. Denote each physics-based model by $f_1(\\xvec),\\ldots,f_K(\\xvec)$. For simplicity, we assume each $f_k(\\boldsymbol{x})$ is implemented as a deterministic simulator with calibrated parameters.\n",
    "\n",
    "The BART-BMM model combines the known physics in each simulator using a mean-based approach. Given observational data $Y_1,\\ldots,Y_n$ at p-dimensional inputs $\\xvec_1,\\ldots,\\xvec_n$, the BMM model is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "  Y(\\xvec_i) = \\sum_{k=1}^K w_k(\\xvec_i)f_k(\\xvec_i) + \\epsilon_i, \\quad \\epsilon_i \\stackrel{\\small{\\text{ind}}}{\\sim} N(0,\\sigma^2)\n",
    "\\end{equation}\n",
    "for $i = 1,\\ldots,n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXs-13Ad_2lb"
   },
   "source": [
    "### **Overview of Tree Models**\n",
    "\n",
    "A tree model is a flexible and non-parametric method for estimating an unknown function $f(\\xvec)$ with input vector $\\xvec$. The objective of a tree model is to recursively partition the input space using binary splitting rules of the form $x_v < c$, where $v \\in \\{1,.\\ldots,p \\}$ and $c$ is a cutpoint from a discretized subset of the real numbers. Each partition corresponds to a terminal node and is assigned a constant value, which serves as the prediction of $f(\\xvec)$. An example tree with respect to a 2-dimensional input space is shown below.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIOZ71_-H1BM"
   },
   "source": [
    "![Image of Tree](https://github.com/jcyannotty/Taweret/raw/develop/docs/source/images/tree_split.png)\n",
    "\n",
    "$\\\\[5 pt]$\n",
    "\n",
    "Panel (a) illustrates an example of a tree structure with three terminal nodes $\\eta_{11},\\eta_{21}$, and $\\eta_{31}$. Panel (b) displays the partitions of the input space, with associated terminal node parameters $\\boldsymbol{\\mu}_{b1}$, $b=1,2,3$. Using the tree model, each $\\xvec$ in the input domain is mapped to one of these three values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0if3Sg1MSEOf"
   },
   "source": [
    "### **The Weight Model**\n",
    "\n",
    "Denote the weight vector as $\\wvec(\\xvec) = (w_1(\\xvec),\\ldots,w_K(\\xvec))^\\top$. Then $\\wvec(\\xvec)$ is modeled by\n",
    "\\begin{equation}\n",
    "  \\wvec(\\xvec) = \\sum_{j = 1}^m \\boldsymbol{g}(\\xvec,T_j,M_j)\n",
    "\\end{equation}\n",
    "where $\\boldsymbol{g}(\\xvec,T_j,M_j)$ is the output of the jth tree $T_j$ with the associated set of parameters $M_j$. Main features of the weight functions include:\n",
    "\n",
    "* Implicit regularization via a multivariate Gaussian prior.\n",
    "* Each weight $w_k(\\xvec)$ prefers the interval $[0,1]$, but is not strictly confined to this region.\n",
    "* No sum-to-one constraint is imposed.\n",
    "* Defined using flexible tree bases which are learned using the observational data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRDJfp8NR_Nm"
   },
   "source": [
    "### **Setting Prior Distributions**\n",
    "\n",
    "Two of the priors in the BART model are described below. The descriptions are followed by plots of the prior densities at various settings of the hyperparameters.\n",
    "\n",
    "#### *The weight function prior*\n",
    "\n",
    "Each of the $K$ weight functions are implicitly regularized via a Gaussian prior. Assuming an ensemble of $m$ trees are used to learn the weight functions, the (implicit) prior for each weight is defined by\n",
    "\n",
    "\\begin{equation}\n",
    "    w_l(x) \\mid \\mathcal{T} \\sim N(0.5, m\\tau^2), \\quad l = 1,\\ldots,K,\n",
    "\\end{equation}\n",
    "where $K$ is the number of models under consideration and $\\mathcal{T} = \\{T_1,\\ldots,T_m\\}$ is the set of trees. Meanwhile, $\\tau$ is specified as\n",
    "\\begin{equation}\n",
    "    \\tau = \\frac{1}{2k\\sqrt{m}}\n",
    "\\end{equation}\n",
    "where $k$ is a tuning parameter. Increasing $k$ limits the flexibility of each weight and concentrates the corresponding prior around the central point of 0.5.  \n",
    "\n",
    "#### *The error variance prior*\n",
    "\n",
    "The error variance is assigned a scaled inverse chi-square distribution, denoted by $\\;\\sigma^2 \\sim \\lambda\\nu/\\chi^2_\\nu$. The hyperparameters of $\\nu$ and $\\lambda$ control the shape and scale of the prior respectively. Increasing $\\nu$ will result in a prior with a higher peak, while increasing $\\lambda$ will shift the prior to the right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "executionInfo": {
     "elapsed": 895,
     "status": "ok",
     "timestamp": 1687641456029,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "6K8iUM2IJruW",
    "outputId": "6d8cd26a-e340-4101-c6b4-46fe41aef354"
   },
   "outputs": [],
   "source": [
    "# Create Figure 1: The BART Priors\n",
    "# Define different values for m*tau^2\n",
    "mtau2_vec = 0.25*np.array([1,1/4,1/9,1/16]) # k = 1,2,3,4 respectively\n",
    "\n",
    "# Evaluate the weight prior density of a grid of points\n",
    "w_grid = np.arange(-0.2,1.2,0.001)\n",
    "wprior1 = norm.pdf(w_grid, 0.5, np.sqrt(mtau2_vec[0]))\n",
    "wprior2 = norm.pdf(w_grid, 0.5, np.sqrt(mtau2_vec[1]))\n",
    "wprior3 = norm.pdf(w_grid, 0.5, np.sqrt(mtau2_vec[2]))\n",
    "wprior4 = norm.pdf(w_grid, 0.5, np.sqrt(mtau2_vec[3]))\n",
    "\n",
    "# Now evaluate the variance prior\n",
    "def scaled_inv_chi2(sig2, nu, lam):\n",
    "    den = (lam*nu/2)**(nu/2)/(gamma(nu/2)*sig2**(1+nu/2))*np.exp(-nu*lam/(2*sig2))\n",
    "    return den\n",
    "\n",
    "s_grid = np.arange(0.001,0.1,0.001)\n",
    "sprior1 = scaled_inv_chi2(s_grid, 5, 0.01)\n",
    "sprior2 = scaled_inv_chi2(s_grid, 5, 0.05)\n",
    "sprior3 = scaled_inv_chi2(s_grid, 10, 0.01)\n",
    "sprior4 = scaled_inv_chi2(s_grid, 10, 0.05)\n",
    "\n",
    "\n",
    "# Plot the prior\n",
    "fig, ax = plt.subplots(1,2, figsize = (14,5))\n",
    "ax[0].plot(w_grid, wprior1, color = 'red')\n",
    "ax[0].plot(w_grid, wprior2, color = 'blue')\n",
    "ax[0].plot(w_grid, wprior3, color = 'green')\n",
    "ax[0].plot(w_grid, wprior4, color = 'orange')\n",
    "ax[0].set_title(\"Implicit Weight Prior\")\n",
    "ax[0].set_xlabel(\"$W_j(x)$\")\n",
    "ax[0].set_ylabel(\"Density\")\n",
    "ax[0].legend([\"k=1\",\"k=2\",\"k=3\",\"k=4\"], loc = \"upper right\")\n",
    "\n",
    "# Plot the prior\n",
    "ax[1].plot(s_grid, sprior1, color = 'red')\n",
    "ax[1].plot(s_grid, sprior2, color = 'blue')\n",
    "ax[1].plot(s_grid, sprior3, color = 'green')\n",
    "ax[1].plot(s_grid, sprior4, color = 'orange')\n",
    "ax[1].set_title(\"Error Variance Prior\")\n",
    "ax[1].set_xlabel(\"$\\sigma^2$\")\n",
    "ax[1].set_ylabel(\"Density\")\n",
    "ax[1].legend([\"(5,0.01)\",\"(5,0.5)\",\"(10,0.01)\",\"(10,0.5)\"], loc = \"upper right\", title = \"($\\\\nu$, $\\lambda$)\")\n",
    "\n",
    "fig.suptitle(\"Figure 1: The BART Priors\", size = 20)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RipHFq79D4-p"
   },
   "source": [
    "## **Installation of Ubuntu Package**\n",
    "The Bayesian Additive Regression Tree (BART) approach for model mixing is implemented in C++ and built as a Ubuntu package. The following three steps can be used to download and install the package.\n",
    "\n",
    "1. Download the package from the corresponding github repository. This is done using the `!wget` command.\n",
    "\n",
    "2. Install the package using the `!dpkg` command.\n",
    "\n",
    "3. Reset the library cache using the `!ldconfig` command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1687641461522,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "GDgnBUIlKnj2"
   },
   "outputs": [],
   "source": [
    "!wget -q https://github.com/jcyannotty/OpenBT/raw/main/openbt_mixing0.current_amd64-MPI_Ubuntu_20.04.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8596,
     "status": "ok",
     "timestamp": 1687641471987,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "Vgngr_QnL8YK",
    "outputId": "988612eb-70fa-4077-a58a-7a605453599d"
   },
   "outputs": [],
   "source": [
    "!dpkg -i openbt_mixing0.current_amd64-MPI_Ubuntu_20.04.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2861,
     "status": "ok",
     "timestamp": 1687641474841,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "QjLHLZsqdj_6"
   },
   "outputs": [],
   "source": [
    "!ldconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVA0_-UCFWJS"
   },
   "source": [
    "### **Working with Taweret**\n",
    "First, we need to clone Taweret from github and switch to the develop branch. While doing so, we will also make Taweret our current directory using the cd command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5158,
     "status": "ok",
     "timestamp": 1687641497918,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "xlHwb-XcODti",
    "outputId": "b2fcdb7d-64f3-4a8d-c66e-ac872fe472d0"
   },
   "outputs": [],
   "source": [
    "# Clone the Taweret Repo\n",
    "!git clone https://github.com/jcyannotty/Taweret.git\n",
    "!cd Taweret && git checkout develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTFHwxJoI1Bf"
   },
   "source": [
    "Finally, we can add Taweret to the system path and import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1687641520795,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "2e6tz7hlQfqu"
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/content/Taweret')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1687641521760,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "l9dSmb4uM5jc"
   },
   "outputs": [],
   "source": [
    "# Taweret Imports\n",
    "from Taweret.models.polynomial_models import sin_exp, cos_exp, sin_cos_exp\n",
    "from Taweret.mix.trees import Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ij1H_er6xIH"
   },
   "source": [
    "## **Example**\n",
    "\n",
    "This example demonstrates the BART-BMM model on a 2-dimensional example with two different simulators. Assume the true system is defined by $$f_\\dagger(x) = \\sin(x_1) + \\cos(x_2),$$\n",
    "over the domain $(x_1,x_2) \\in [-\\pi,\\pi]\\times [-\\pi,\\pi]$.  Furthermore, consider a model set which defines the two simulators under consideration in terms of Taylor series expansions of $\\sin(x_1)$ and $\\cos(x_2).$ The simulators are defined by $f_1(x)$ and $f_2(x)$ as shown below:\n",
    "\n",
    "$\\\\[5 pt]$\n",
    "\n",
    "\\begin{align}\n",
    "    f_1(x) &= \\sum_{j=0}^7 \\frac{s^{(j)}(x_1)}{j!}(x_1-\\pi)^j + \\sum_{k=0}^{10} \\frac{c^{(k)}(x_2)}{k!}(x_2-\\pi)^k  \\\\[8 pt]  \n",
    "    f_2(x) &= \\sum_{j=0}^{13} \\frac{s^{(j)}(x_1)}{j!}(x_1+\\pi)^j + \\sum_{k=0}^6 \\frac{c^{(k)}(x_2)}{k!}(x_2+\\pi)^k.\n",
    "\\end{align}\n",
    "\n",
    "$\\\\[5 pt]$\n",
    "\n",
    "where $x = (x_1,x_2)$, $s^{(j)}(x_1)$ denotes the jth derivative of $\\sin(x_1)$, and $c^{(k)}(x_1)$ denotes the kth derivative of $\\cos(x_2)$.\n",
    "\n",
    "The simulator, $f_1(x)$ involves Taylor series expansions of $\\sin(x_1)$ and $\\cos(x_2)$ about $\\pi$. Meanwhile, $f_2(x)$ is constructed using two Taylor series expansions about $-\\pi$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avkOs41jJBjq"
   },
   "source": [
    "### **Training Data**\n",
    "Prior to mixing, we can generate training data over a two-dimensional grid. This is done by first generating a set of training inputs using the `grid_2d_design(...)` function. The function generates points with a space filling design. The 2-dimensional input space is divided into a $n_1 \\times n_2$ grid. Then, an input $\\xvec = (x_1,x_2)$ is randomly generated in each rectangle of the grid. This results in a total of $n = n_1n_2$ training points.\n",
    "\n",
    "The training points are passed through the true function of $f_\\dagger(\\xvec) = \\sin(x_1) + \\cos(x_2)$ and random noise is added to simulate observational data, $Y_1,\\ldots,Y_n$. The example below generates data from 80 training points, though one can easily use a smaller or larger training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1687641526943,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "-RJZbDhDGOhp"
   },
   "outputs": [],
   "source": [
    "## Functions for design points\n",
    "# n1 = number of bins in the x1 dimension\n",
    "# n2 = number of bins in the x2 dimension\n",
    "# n = n1*n2 is the total training size\n",
    "def grid_2d_design(n1,n2, xmin = [-1,-1], xmax = [1,1]):\n",
    "  # Generate n uniform rvs\n",
    "  n = n1*n2\n",
    "  ux = np.random.uniform(0,1,n)\n",
    "  uy = np.random.uniform(0,1,n)\n",
    "\n",
    "  # Dimensions for each rectangle\n",
    "  x1_len = (xmax[0] - xmin[0])/n1\n",
    "  x2_len = (xmax[1] - xmin[1])/n2\n",
    "  xgrid = [[x, y] for x in range(n1) for y in range(n2)]\n",
    "  xgrid = np.array(xgrid).transpose()\n",
    "\n",
    "  # Get points\n",
    "  x1 = ux*x1_len + x1_len*xgrid[0] + xmin[0]\n",
    "  x2 = uy*x2_len + x2_len*xgrid[1] + xmin[1]\n",
    "\n",
    "  # Join data\n",
    "  xdata = np.array([x1,x2]).transpose()\n",
    "  return xdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGykT6UB7adv"
   },
   "source": [
    "The training inputs are generated over the rectangle $[-\\pi,\\pi]^2$. The design points are shown in the figure below. Then, the observational data is independently generated according to\n",
    "\\begin{equation}\n",
    "    Y_i \\sim  N(f_\\dagger(x_i),\\sigma^2)\n",
    "\\end{equation}\n",
    "where $x_i = (x_{i1},x_{i2})$ is the ith training input for $i=1,\\ldots,n$. In this example, $n = 80$ and $\\sigma = 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1687641529933,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "-KCYTCh9GT1_"
   },
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "# Pick the grid dimensions\n",
    "nx1 = 10; nx2 = 8\n",
    "\n",
    "# Total training size\n",
    "n_train = nx1*nx2\n",
    "\n",
    "# Generate inputs\n",
    "x_train = grid_2d_design(nx1,nx2,[-np.pi,-np.pi],[np.pi,np.pi])\n",
    "\n",
    "# Generate observational data with true standard deviation of 0.1\n",
    "f0_train = np.sin(x_train.transpose()[0]) + np.cos(x_train.transpose()[1])\n",
    "y_train = f0_train + np.random.normal(0,0.1,n_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jeZXkcg7hVr"
   },
   "source": [
    "The design of the experiment is shown below. Clearly the training points are spread out across the entire domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1687641532775,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "xMV8sQzVGw2C",
    "outputId": "ff6c8298-d75b-433b-e58f-0ab6b0b89fa8"
   },
   "outputs": [],
   "source": [
    "# Create Figure 2: The Training inputs\n",
    "plt.scatter(x_train.transpose()[0],x_train.transpose()[1])\n",
    "plt.title(\"Figure 2: Training Inputs\", size = 20)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-SVxlLR74Ly"
   },
   "source": [
    "### **The Model Set**\n",
    "\n",
    "Now we can visualize the true function and the two simulators under consideration. Each surface is shown below. Note, the plots are truncated in areas where an expansion rapidly increases or decreases. This is done only for visual purposes.\n",
    "\n",
    "The first plot illustrates the true surface. This surface has various peaks and valleys across the input domain. We will see each simulator can be used to explain one of these features of the true surface. The plot is generated by evaluating $f_\\dagger(x)$ over a dense grid of points across the input domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "executionInfo": {
     "elapsed": 630,
     "status": "ok",
     "timestamp": 1687641566681,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "GMqyiSa8G6e5",
    "outputId": "a4335b56-d297-4c66-9b85-4676432582cf"
   },
   "outputs": [],
   "source": [
    "# Create Figure 3: The true surface\n",
    "# Plot the surfaces\n",
    "n_test = 30\n",
    "x1_test = np.outer(np.linspace(-np.pi, np.pi, n_test), np.ones(n_test))\n",
    "x2_test = x1_test.copy().transpose()\n",
    "f0_test = (np.sin(x1_test) + np.cos(x2_test))\n",
    "x_test = np.array([x1_test.reshape(x1_test.size,),x2_test.reshape(x1_test.size,)]).transpose()\n",
    "\n",
    "# Define color map\n",
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "# Creating figure\n",
    "fig = plt.figure(figsize =(11, 7))\n",
    "ax = plt.axes(projection ='3d')\n",
    "\n",
    "# Creating plot\n",
    "ax.plot_surface(x1_test, x2_test, f0_test, cmap = cmap, vmin = -2, vmax = 2)\n",
    "plt.title(\"Figure 3: True System\", size = 20)\n",
    "plt.xlabel(\"$x_1$\", size = 14)\n",
    "plt.ylabel(\"$x_2$\", size = 14)\n",
    "ax.set_zlim([-2,2])\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6amrDi4X79y9"
   },
   "source": [
    "Now we can view the two simulators we wish to mix. The first simulator is comprised of two Taylor series expansions centered about $\\pi$, hence it provides higher fidelity predictions in the upper right corner of the domain. The second simulator is comprised of two Taylor series expansions centered about $-\\pi$. Additionally, its Taylor series expansion of $\\sin(x)$ is a high-fidelity approximation across the entire interval of $[-\\pi,\\pi]$. Hence $f_2(x)$ provides higher fidelity predictions in the bottom half of the domain.\n",
    "\n",
    "The surfaces produced by each simulator are shown below.\n",
    "\n",
    "#### **The $f_1(x)$ surface:**\n",
    "\n",
    "The first expansion is shown below. The predicted surface is accurate for points $(x_1,x_2)$ close to the point $(\\pi,\\pi)$. Based on the plot below, we can see the first expansion is designed to approximate the peak of the true system in this upper right corner of the domain. Meanwhile, the prediction becomes less accurate as $x_1$ or $x_2$ moves away from $\\pi$. Note for visual purposes, the plot below truncates the approximation in these less accurate regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "executionInfo": {
     "elapsed": 1635,
     "status": "ok",
     "timestamp": 1687641571618,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "ziOVW1qFHlBB",
    "outputId": "c71663d2-2967-4c09-d701-bc7f5a3aa853"
   },
   "outputs": [],
   "source": [
    "# Create Figure 4: The f1 surface\n",
    "# Plot the first simulator\n",
    "sin7 = sin_exp(7,np.pi)\n",
    "cos10 = cos_exp(10,np.pi)\n",
    "\n",
    "f1_sin = sin7.evaluate(x1_test.transpose()[0])[0]\n",
    "f1_cos = cos10.evaluate(x1_test.transpose()[0])[0]\n",
    "\n",
    "f1_sin_grid = np.outer(f1_sin, np.ones(n_test))\n",
    "f1_cos_grid = np.outer(f1_cos, np.ones(n_test)).transpose()\n",
    "\n",
    "f1_test = f1_sin_grid + f1_cos_grid\n",
    "\n",
    "# Subset the data for the plot (only for visualization)\n",
    "f1_test_filter = f1_test.copy()\n",
    "for i in range(n_test):\n",
    "    f1_test_filter[i][np.where(f1_test_filter[i]>3)] = 3.05\n",
    "    f1_test_filter[i][np.where(f1_test_filter[i]<-3)] = -3.05\n",
    "\n",
    "# Creating figure\n",
    "fig = plt.figure(figsize =(11, 7))\n",
    "ax = plt.axes(projection ='3d')\n",
    "\n",
    "# Creating plot\n",
    "ax.plot_surface(x2_test, x1_test, f1_test_filter, cmap = cmap, vmin = -2, vmax = 2)\n",
    "ax.set_zlim([-3,3])\n",
    "ax.set_xlim([-np.pi,np.pi])\n",
    "ax.set_ylim([-np.pi,np.pi])\n",
    "plt.title(\"Figure 4: $f_1(x)$ - (truncated range)\", size = 20)\n",
    "plt.xlabel(\"$x_2$\", size = 14)\n",
    "plt.ylabel(\"$x_1$\", size = 14)\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LN0qRw2W8P6S"
   },
   "source": [
    "#### **The $f_2(x)$ surface**\n",
    "\n",
    "The second expansion is shown below. The predicted surface is accurate for points $(x_1,x_2)$ close to the point $(-\\pi,-\\pi)$. Additionally, the expansion of $\\sin(x_1)$ is a high-fidelity approximation of $\\sin(x_1)$ across the interval $[-\\pi,\\pi]$. Hence, from the plot below, $f_2(x)$ can be used to approximate the valley and curvature in the true function for points in the bottom region of the domain. Once again, the surface is truncated for visual purposes in regions where the expansion rapidly increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "executionInfo": {
     "elapsed": 1081,
     "status": "ok",
     "timestamp": 1687641576031,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "hQSTNU1OIEAk",
    "outputId": "6497b166-e01f-4a44-8c86-3641dc9c3868"
   },
   "outputs": [],
   "source": [
    "# Create Figure 5: The f2 surface\n",
    "# Plot the second simulator\n",
    "sin13 = sin_exp(13,-np.pi)\n",
    "cos6 = cos_exp(6,-np.pi)\n",
    "\n",
    "f2_sin = sin13.evaluate(x1_test.transpose()[0])[0]\n",
    "f2_cos = cos6.evaluate(x1_test.transpose()[0])[0]\n",
    "\n",
    "f2_sin_grid = np.outer(f2_sin, np.ones(n_test))\n",
    "f2_cos_grid = np.outer(f2_cos, np.ones(n_test)).transpose()\n",
    "\n",
    "f2_test = f2_sin_grid + f2_cos_grid\n",
    "\n",
    "# Subset the data for the plot (only for visualization)\n",
    "f2_test_filter = f2_test.copy()\n",
    "for i in range(n_test):\n",
    "    f2_test_filter[i][np.where(f2_test_filter[i]>3)] = 3.05\n",
    "    f2_test_filter[i][np.where(f2_test_filter[i]<-3)] = -3.05\n",
    "\n",
    "\n",
    "# Creating figure\n",
    "fig = plt.figure(figsize =(11, 7))\n",
    "ax = plt.axes(projection ='3d')\n",
    "\n",
    "# Creating plot\n",
    "ax.set_zlim([-4,2])\n",
    "ax.plot_surface(x1_test, x2_test, f2_test_filter,cmap = cmap, vmin = -2, vmax = 2)\n",
    "plt.title(\"Figure 5: $f_2(x)$ - (truncated range)\", size = 20)\n",
    "plt.xlabel(\"$x_1$\", size = 14)\n",
    "plt.ylabel(\"$x_2$\", size = 14)\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5KUXO5pjC7A"
   },
   "source": [
    "Alternatively, we can view the accuracy of each individual Taylor Series expansions. The first two expansions (red) are added together to obtain the first simulator, while the second two expansions (blue) are added together to obtain the second simulator. The following observations are made:\n",
    "\n",
    "* The first row of plots suggest the first simulator, $f_1(x)$, is an accurate approximation of the true function for positive $x_1$ and $x_2$.\n",
    "\n",
    "* The second row of plots suggest the second simulator, $f_2(x)$, is an accurate approximation of the true function for negative $x_2$ and any values of $x_1$.\n",
    "\n",
    "Hence, we expect $f_1(x)$ to be influential in the upper right corner of the domain and $f_2(x)$ to be influential in the bottom half of the domain. These results should be reflected in the final posterior weights for the BMM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 967
    },
    "executionInfo": {
     "elapsed": 1371,
     "status": "ok",
     "timestamp": 1687641581491,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "poaZaXtXjMK9",
    "outputId": "055b330e-2712-46fb-f010-dc9781f89be6"
   },
   "outputs": [],
   "source": [
    "# Create Figure 6: Taylor Series Expansions\n",
    "# Store the 4 individual Taylor series expansions and plot titles\n",
    "exp_list = [f1_sin, f1_cos, f2_sin, f2_cos]\n",
    "exp_title = [\"7th Order Expansion of $\\sin(x_1)$ about $\\pi$\",\"10th Order Expansion of $\\cos(x_2)$ about $\\pi$\",\\\n",
    "             \"13th Order Expansion of $\\sin(x_1)$ about $-\\pi$ \", \"6th Order Expansion of $\\cos(x_2)$ about $-\\pi$\"]\n",
    "\n",
    "# Plot the marginal effects of each expansion\n",
    "k = 0\n",
    "fig, ax = plt.subplots(2,2,figsize = (12,10))\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        exp_color = 'red'\n",
    "    else:\n",
    "        exp_color = 'blue'\n",
    "    for j in range(2):\n",
    "        if j == 0:\n",
    "            ax[i][j].plot(x1_test.transpose()[0], np.sin(x1_test.transpose()[0]), color = 'black')\n",
    "            exp_label = \"$\\sin(x_1)$\"\n",
    "        else:\n",
    "            ax[i][j].plot(x1_test.transpose()[0], np.cos(x1_test.transpose()[0]), color = 'black')\n",
    "            exp_label = \"$\\cos(x_2)$\"\n",
    "\n",
    "        ax[i][j].plot(x1_test.transpose()[0], exp_list[k], color = exp_color)\n",
    "        ax[i][j].set_ylim(-5,5)\n",
    "        ax[i][j].set_ylabel(exp_label)\n",
    "        ax[i][j].set_xlabel(\"$x_\"+str(j+1)+\"$\")\n",
    "        ax[i][j].set_title(exp_title[k], size = 12)\n",
    "        ax[i][j].legend([exp_label, \"Taylor Series\"],loc = \"lower right\")\n",
    "        # Increment total plot index\n",
    "        k = k + 1\n",
    "fig.suptitle(\"Figure 6: Taylor Series Expansions\", size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgNQbU7RKSM2"
   },
   "source": [
    "## **The BART-BMM Model**\n",
    "\n",
    "The BART-BMM model is trained using the following steps.\n",
    "\n",
    "1. Define the model set using the three lines of code shown below. The first two lines define a class instance for each Taylor series expansion. The third line of code defines the model set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1687641585685,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "jKDZ_72nKQau"
   },
   "outputs": [],
   "source": [
    "# Define the model set\n",
    "f1 = sin_cos_exp(7,10,np.pi,np.pi) # 7th order sin(x1) + 10th order cos(x2)\n",
    "f2 = sin_cos_exp(13,6,-np.pi,-np.pi) # 13th order sin(x1) + 6th order cos(x2)\n",
    "model_dict = {'model1':f1, 'model2':f2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4zuI8rokmz6"
   },
   "source": [
    "2. Define the class instance of the BART-BMM model using the `Trees` class. For this example, the class instance is called `mix`.\n",
    "\n",
    "3. Set the prior information using the `set_prior()` method. One can see the affect of $k$ and $\\nu$ in Figure 1. Also note, $\\lambda = \\text{overallsd}^2$.\n",
    "\n",
    "4. Fit the model using the `train()`. This requires the user to pass in the data and relevant MCMC arguments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70085,
     "status": "ok",
     "timestamp": 1687641657654,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "6JE3_y7ZKdQ7",
    "outputId": "e0308691-3a86-4ebf-d8d2-2d0f5d0d6142"
   },
   "outputs": [],
   "source": [
    "# Fit the BMM Model\n",
    "# Initialize the Trees class instance\n",
    "mix = Trees(model_dict = model_dict, google_colab = True)\n",
    "\n",
    "# Set prior information\n",
    "mix.set_prior(k=2.0,ntree=30,overallnu=5,overallsd=0.01,inform_prior=False)\n",
    "\n",
    "# Train the model\n",
    "fit = mix.train(X=x_train, y=y_train, ndpost = 5000, nadapt = 2000, nskip = 1000, adaptevery = 200, minnumbot = 4, tc = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YFpWxYQktsJ"
   },
   "source": [
    "5. Obtain the predictions from the mixed function and the corresponding weight functions using the methods `predict()` and `predict_weights()`, respectively. Both methods require an array of test points and a confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41341,
     "status": "ok",
     "timestamp": 1687641698967,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "HQdFHFYle-QA"
   },
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "ppost, pmean, pci, pstd = mix.predict(X = x_test, ci = 0.95)\n",
    "wpost, wmean, wci, wstd = mix.predict_weights(X = x_test, ci = 0.95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_eUDbDxlLGp"
   },
   "source": [
    "### **Predictions**\n",
    "\n",
    "The predicted system can be displayed by executing the following three code chunks. The first chunk displays the mean predicted surface while the second compares the mean  prediction with the true system using a heat map. The third chunk displays the mean residuals.  \n",
    "\n",
    "#### **Questions:**\n",
    "\n",
    "**Q1.** *What region(s) of the domain result in an accurate approximation of the true system? What region(s) of the domain are not accurately predicted by the BMM model? Do these results make sense, given the strengths and weaknesses of each expansion?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "executionInfo": {
     "elapsed": 1471,
     "status": "ok",
     "timestamp": 1687641703330,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "Jus1y4CjfN9V",
    "outputId": "b78e339b-884a-4894-f82e-80a53eaf7067"
   },
   "outputs": [],
   "source": [
    "# Create Figure 7: The true vs. predicted surface\n",
    "fig = plt.figure(figsize = (16,8))\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.plot_surface(x1_test, x2_test, f0_test, cmap = cmap, vmin = -2, vmax = 2)\n",
    "ax.set_title(\"True System\", size = 14)\n",
    "ax.set_xlabel(\"$x_1$\", size = 14)\n",
    "ax.set_ylabel(\"$x_2$\", size = 14)\n",
    "ax.set_zlim([-2.5,2.5])\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.plot_surface(x1_test, x2_test, pmean.reshape(x1_test.shape), cmap = cmap, vmin = -2, vmax = 2)\n",
    "ax.set_title(\"Predicted System\", size = 14)\n",
    "ax.set_xlabel(\"$x_1$\", size = 14)\n",
    "ax.set_ylabel(\"$x_2$\", size = 14)\n",
    "ax.set_zlim([-2.5,2.5])\n",
    "\n",
    "fig.suptitle(\"Figure 7: True versus Predicted System\", size = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7_9oqoLlSfB"
   },
   "source": [
    "Another way to visually compare the posterior mean prediction and the true system is by a heat map, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "executionInfo": {
     "elapsed": 921,
     "status": "ok",
     "timestamp": 1687641708077,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "Jh6zEgPdgys9",
    "outputId": "d630e1bf-376d-4afd-cdf8-354b787e6418"
   },
   "outputs": [],
   "source": [
    "# Create Figure 8: The true vs. predicted system as heat maps\n",
    "# Heat map comparing the surfaces\n",
    "fig, ax = plt.subplots(1,2, figsize = (12,5))\n",
    "\n",
    "pcm1 = ax[0].pcolormesh(f0_test.transpose(),cmap = cmap, vmin = -2.5, vmax = 2.5)\n",
    "ax[0].set_title(\"True System\", size = 16)\n",
    "ax[0].set(xlabel = \"$x_1$\", ylabel = \"$x_2$\")\n",
    "ax[0].xaxis.set_major_locator(ticker.FixedLocator(np.round(np.linspace(0, n_test, 6),3)))\n",
    "ax[0].xaxis.set_major_formatter(ticker.FixedFormatter(np.round(np.linspace(-np.pi, np.pi, 6),3)))\n",
    "ax[0].yaxis.set_major_locator(ticker.FixedLocator(np.round(np.linspace(0, n_test, 6),3)))\n",
    "ax[0].yaxis.set_major_formatter(ticker.FixedFormatter(np.round(np.linspace(-np.pi, np.pi, 6),3)))\n",
    "fig.colorbar(pcm1,ax = ax[0])\n",
    "\n",
    "\n",
    "# Predicted mean\n",
    "pcm2 = ax[1].pcolormesh(pmean.reshape(x1_test.shape).transpose(),cmap = cmap, vmin = -2.5, vmax = 2.5)\n",
    "ax[1].set_title(\"Posterior Mean Prediction\", size = 16)\n",
    "ax[1].set(xlabel = \"$x_1$\", ylabel = \"$x_2$\")\n",
    "ax[1].xaxis.set_major_locator(ticker.FixedLocator(np.round(np.linspace(0, n_test, 6),3)))\n",
    "ax[1].xaxis.set_major_formatter(ticker.FixedFormatter(np.round(np.linspace(-np.pi, np.pi, 6),3)))\n",
    "ax[1].yaxis.set_major_locator(ticker.FixedLocator(np.round(np.linspace(0, n_test, 6),3)))\n",
    "ax[1].yaxis.set_major_formatter(ticker.FixedFormatter(np.round(np.linspace(-np.pi, np.pi, 6),3)))\n",
    "\n",
    "fig.colorbar(pcm2,ax = ax[1])\n",
    "fig.suptitle(\"Figure 8: True versus Predicted System\", size = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84fmJepIlZTV"
   },
   "source": [
    "Finally, we can plot the mean residual, which is defined as $\\hat{r}(x) = f_\\dagger(x) - \\hat{f}_\\dagger(x)$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1687641715577,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "sr0Vh0-1g6aV",
    "outputId": "ca1a5f93-6c71-4630-f64f-60b44eed3372"
   },
   "outputs": [],
   "source": [
    "# Create Figure 9: Posterior Mean resiudals\n",
    "cmap_rb = plt.get_cmap(\"RdBu\")\n",
    "fig, ax = plt.subplots(1,1, figsize = (6,5))\n",
    "\n",
    "pcm1 = ax.pcolormesh((f0_test - pmean.reshape(x1_test.shape)).transpose(),cmap = cmap_rb, vmin = -2.5, vmax = 2.5)\n",
    "ax.set_title(\"Figure 9: Posterior Mean Residuals\", size = 14)\n",
    "ax.set(xlabel = \"$x_1$\", ylabel = \"$x_2$\")\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(np.round(np.linspace(0, n_test, 6),3)))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(np.round(np.linspace(-np.pi, np.pi, 6),3)))\n",
    "ax.yaxis.set_major_locator(ticker.FixedLocator(np.round(np.linspace(0, n_test, 6),3)))\n",
    "ax.yaxis.set_major_formatter(ticker.FixedFormatter(np.round(np.linspace(-np.pi, np.pi, 6),3)))\n",
    "\n",
    "fig.colorbar(pcm1,ax = ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8T0np6RPlegm"
   },
   "source": [
    "### **Weight Functions**\n",
    "\n",
    "Now, we can plot the mean weight functions from the BMM model using the code below. The weight functions help identify where each simulator is accurate or inaccurate. The mean weight functions are shown in Figure 10 and the sum of the mean weight functions is shown in Figure 11.\n",
    "\n",
    "#### **Questions:**\n",
    "\n",
    "**Q2.** *Based on the weight functions, in what region of the domain is the first simulator highly influential in the BMM prediction? Meanwhile, in what regions is the first simulator not influential in the BMM prediction?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLfrth9_vgKQ"
   },
   "source": [
    "**Q3.** *Based on the weight functions, in what region of the domain is the second simulator highly influential in the BMM prediction? Meanwhile, in what regions is the second simulator not influential in the BMM prediction?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6fzeVhVvkCd"
   },
   "source": [
    "**Q4.** *Using Figure 11, do you see any similarities between the sum of the mean weights and the resiudal plot in Figure 9? Considering what we know about each simulator, what might this suggest about the sum of the weight functions?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0ye-8F-vpcS"
   },
   "source": [
    "**Q5.** *Given the design of each simulator from Figures 3-6, do these results make sense?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1687641723653,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "BMfHFuwDhM6d",
    "outputId": "85e85abe-41b3-43a1-8aed-0041fa4a2618"
   },
   "outputs": [],
   "source": [
    "# Create Figure 10: The posterior mean weight functions\n",
    "cmap_hot = plt.get_cmap('hot')\n",
    "w1 = wmean.transpose()[0]\n",
    "w2 = wmean.transpose()[1]\n",
    "\n",
    "w1_mean = wmean.transpose()[0]\n",
    "w1_mean = w1_mean.reshape(x1_test.shape).transpose()\n",
    "\n",
    "w2_mean = wmean.transpose()[1]\n",
    "w2_mean = w2_mean.reshape(x1_test.shape).transpose()\n",
    "\n",
    "w_sum = w1_mean + w2_mean\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (12,5))\n",
    "pcm0 = ax[0].pcolormesh(w1_mean,cmap = cmap_hot, vmin = -0.05, vmax = 1.05)\n",
    "ax[0].set_title(\"Posterior Mean of $w_1(x)$\", size = 14)\n",
    "ax[0].set(xlabel = \"$x_1$\", ylabel = \"$x_2\")\n",
    "ax[0].xaxis.set_major_locator(ticker.FixedLocator(np.round(np.linspace(0, n_test, 6),3)))\n",
    "ax[0].xaxis.set_major_formatter(ticker.FixedFormatter(np.round(np.linspace(-np.pi, np.pi, 6),3)))\n",
    "ax[0].yaxis.set_major_locator(ticker.FixedLocator(np.round(np.linspace(0, n_test, 6),3)))\n",
    "ax[0].yaxis.set_major_formatter(ticker.FixedFormatter(np.round(np.linspace(-np.pi, np.pi, 6),3)))\n",
    "fig.colorbar(pcm0,ax = ax[0])\n",
    "\n",
    "pcm1 = ax[1].pcolormesh(w2_mean,cmap = cmap_hot, vmin = -0.05, vmax = 1.05)\n",
    "ax[1].set_title(\"Posterior Mean of $w_2(x)$\", size = 14)\n",
    "ax[1].set(xlabel = \"$x_1$\", ylabel = \"$x_2$\")\n",
    "ax[1].xaxis.set_major_locator(ticker.FixedLocator(np.round(np.linspace(0, n_test, 6),3)))\n",
    "ax[1].xaxis.set_major_formatter(ticker.FixedFormatter(np.round(np.linspace(-np.pi, np.pi, 6),3)))\n",
    "ax[1].yaxis.set_major_locator(ticker.FixedLocator(np.round(np.linspace(0, n_test, 6),3)))\n",
    "ax[1].yaxis.set_major_formatter(ticker.FixedFormatter(np.round(np.linspace(-np.pi, np.pi, 6),3)))\n",
    "fig.colorbar(pcm1,ax = ax[1])\n",
    "fig.suptitle(\"Figure 10: Posterior Mean Weight Functions\", size = 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 769,
     "status": "ok",
     "timestamp": 1687641728502,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "yrgmIa6ed3PJ",
    "outputId": "e30deef2-1aa2-417b-fa92-516c9a258d38"
   },
   "outputs": [],
   "source": [
    "# Create Figure 11: The posterior mean of the sum of the weight functions\n",
    "fig, ax = plt.subplots(1,1, figsize = (7,5))\n",
    "\n",
    "pcm2 = ax.pcolormesh(w_sum,cmap = cmap_hot, vmin = 0, vmax = 1.15)\n",
    "ax.set_title(\"Figure 11: Posterior Mean of \\n $w_1(x) + w_2(x)$\", size = 18)\n",
    "ax.set(xlabel = \"$X_1$\", ylabel = \"$X_2$\")\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(np.round(np.linspace(0, n_test, 6),3)))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(np.round(np.linspace(-np.pi, np.pi, 6),3)))\n",
    "ax.yaxis.set_major_locator(ticker.FixedLocator(np.round(np.linspace(0, n_test, 6),3)))\n",
    "ax.yaxis.set_major_formatter(ticker.FixedFormatter(np.round(np.linspace(-np.pi, np.pi, 6),3)))\n",
    "fig.colorbar(pcm2,ax = ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6h5UR3tnmOty"
   },
   "source": [
    "### **Error Standard Deviation**\n",
    "Finally, the posterior distribution of the error standard deviation is shown below. The true value of $\\sigma = 0.10$.\n",
    "\n",
    "#### **Questions:**\n",
    "\n",
    "**Q6.** *Does the BMM model accurately estimate $\\sigma$?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 606,
     "status": "ok",
     "timestamp": 1687641740066,
     "user": {
      "displayName": "Alexandra Semposki",
      "userId": "00880540762960265989"
     },
     "user_tz": 240
    },
    "id": "GhlYzt52mQac",
    "outputId": "875c708e-677c-4001-ceda-dc0c17dab85f"
   },
   "outputs": [],
   "source": [
    "# Plot the posterior of the error standard deviation\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "plt.hist(mix.posterior, zorder = 2)\n",
    "plt.title(\"Figure 12: Posterior Error Standard Deviation\")\n",
    "plt.xlabel(\"$\\sigma$\") # Update Label\n",
    "plt.ylabel(\"Frequency\") # Update Label\n",
    "plt.grid(True, color='lightgrey', zorder = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4eeEPY3SwzU"
   },
   "source": [
    "---\n",
    "## * * * Challenge time! * * *\n",
    "\n",
    "Now it's time to use BART to model mix across the nuclear chart! Since this involves both dimensions, $N$ and $Z$, this will be much more realistic than in Challenge Project 1. Again, we have an outline for the code written out below, but we'll be leaving most of it up to you. Good luck!\n",
    "\n",
    "__Some tips__:\n",
    "- Your model wrapper class(es) must use all of the functions in the BaseModel class, since it is an abstract base class, even if they are not implemented, so make sure to define all of the models\n",
    "- Taweret requires arrays when using Multivariate, so you'll need to convert your dataframe columns into numpy arrays using to_numpy()\n",
    "- Taweret's BaseModel evaluate function requires you to send in the input space array even if it is not directly being used, so don't forget to make that an argument in your evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJlVrln4V6Rn"
   },
   "outputs": [],
   "source": [
    "# step 1: define the models from the nuclear mass dataframe\n",
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOu76UgkSlbl"
   },
   "outputs": [],
   "source": [
    "# step 2: make the dict of models for Taweret\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htva3gD6QX3S"
   },
   "outputs": [],
   "source": [
    "# step 3: create a new instance of the Trees class, set the prior,\n",
    "# and train the model\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dumWOR_BQqcw"
   },
   "outputs": [],
   "source": [
    "## step 4: get the predictions of the system and the weight functions here\n",
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpiWS6PgUNRj"
   },
   "outputs": [],
   "source": [
    "## step 5: plot the heat map of the predictions here\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orhJeyc3UKMo"
   },
   "source": [
    "__Question__: Do your results look reasonable given the plots you made of the individual models at the beginning of the challenge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MYiDgiTUSw1"
   },
   "outputs": [],
   "source": [
    "## step 6: plot the residuals here\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tRlZCqXUb2H"
   },
   "outputs": [],
   "source": [
    "## step 7: plot the mean weight functions here\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGKNKY9QUY_p"
   },
   "source": [
    "__Question__: Do your weights look reasonable? What do they indicate about the model mixing across the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwSp-UOdUfno"
   },
   "source": [
    "---\n",
    "---\n",
    "BART code, explanations, toy example, questions, and step-by-step process for the challenge project written by: John Yannotty\n",
    "\n",
    "Challenge project conceived and written by: Alexandra Semposki\n",
    "\n",
    "Nuclear mass data supplied by: Rahul Jain"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
